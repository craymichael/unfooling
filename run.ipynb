{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from unfooling.pipeline import evaluate_detector\n",
    "from unfooling.pipeline import generate_explanations\n",
    "from unfooling.pipeline import load_experiment_and_data\n",
    "from unfooling.pipeline import compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configuration\n",
    "Define the experiment name, the defense approach, and a few other settings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C:  # Config\n",
    "    experiment_name = 'COMPAS'\n",
    "    detector_name = 'KNNCAD'\n",
    "    detect_proba = False\n",
    "    test_size = 0.1\n",
    "    debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Experiment\n",
    "Load the experiment problem definition and its data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3735c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = load_experiment_and_data(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Explanations\n",
    "For each explainer, generate explanations with and without the ``fooling'' adversarial attack(s)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324cadfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explainer_data = generate_explanations(C, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defense Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795dc80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "C.detector_name = 'KNNCAD'\n",
    "hparams = dict(\n",
    "    distance_agg='max',\n",
    "    metric='minkowski',\n",
    "    epsilon=0.1,\n",
    "    n_neighbors=15,\n",
    "    p=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "print(f'Using hparams for {C.detector_name}:')\n",
    "pprint(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of the Defense\n",
    "Here, the defense approach is evaluated on the explainers with and without the adversarial attack(s)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_explainer_samples = len(P.X_train) * 10\n",
    "print('n_explainer_samples', n_explainer_samples)\n",
    "results, detectors = evaluate_detector(C, P, explainer_data, hparams,\n",
    "                                       n_explainer_samples=n_explainer_samples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Detection Evaluation Metrics\n",
    "The gathered results for attack detection are shown in the subsequent blocks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd371c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replace_strs = {\n",
    "    'delta': 'Î”',\n",
    "    'explainer': 'expl',\n",
    "    'pct': '%',\n",
    "    'threshold': 'thresh',\n",
    "    'robust': 'R',\n",
    "    'greater': '>',\n",
    "    'under': '<',\n",
    "    'normalized': 'norm',\n",
    "}\n",
    "\n",
    "scores = []\n",
    "for result in results:\n",
    "    score = compute_metrics(result)\n",
    "    for k, v in [*score.items()]:\n",
    "        k_orig = k\n",
    "        for a, b in replace_strs.items():\n",
    "            k = k.replace(a, b)\n",
    "        score[k] = score.pop(k_orig)\n",
    "    score.update(\n",
    "        explainer=result.meta.explainer,\n",
    "        innocuous_model=result.meta.innocuous_model,\n",
    "    )\n",
    "    scores.append(score)\n",
    "\n",
    "score_df = pd.DataFrame(scores)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for explainer, explainer_score_df in score_df.groupby('explainer'):\n",
    "    score_map = dict(tuple(explainer_score_df.groupby('innocuous_model')))\n",
    "    for task, expl_score_df in explainer_score_df.groupby('innocuous_model'):\n",
    "        fidelity_task = expl_score_df['cdf_Î”_expl_test'].values[0]\n",
    "        print('cdf_Î”', explainer, task, fidelity_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_features = P.problem.biased_features\n",
    "n_feats = P.X_test.shape[1]\n",
    "\n",
    "for explainer, expl_expl_data in explainer_data.items():\n",
    "    for task, expl_expl_task_data in expl_expl_data.items():\n",
    "        explanations = expl_expl_task_data['explanations']\n",
    "        y_test_pred_f = expl_expl_task_data['y_test_pred_f_biased']\n",
    "        if y_test_pred_f is None:\n",
    "            y_test_pred_f = expl_expl_task_data['y_test_pred']\n",
    "        score = 0\n",
    "        for yi, expl in zip(y_test_pred_f, explanations):\n",
    "            expl = {k.rsplit('=', 1)[0]: v for k, v in expl}\n",
    "            # ascending\n",
    "            expl_keys_asc = sorted(expl.keys(), key=lambda x: expl[x])\n",
    "            f_ranks = []\n",
    "            expl_ranks = []\n",
    "            for feat in biased_features:\n",
    "                f_ranks.append(n_feats - 1)\n",
    "                try:\n",
    "                    expl_ranks.append(expl_keys_asc.index(feat))\n",
    "                except ValueError:\n",
    "                    expl_ranks.append(0)\n",
    "            for feat in biased_features:\n",
    "                rank_f = n_feats - 1\n",
    "                try:\n",
    "                    rank = expl_keys_asc.index(feat)\n",
    "                except ValueError:\n",
    "                    rank = 0\n",
    "                if yi == 0:\n",
    "                    rank_f = n_feats - rank_f\n",
    "                    rank = n_feats - rank\n",
    "                f_ranks.append(rank_f)\n",
    "                expl_ranks.append(rank)\n",
    "            for feat in {*P.features} - {*biased_features}:\n",
    "                rank_f = 0\n",
    "                try:\n",
    "                    rank = expl_keys_asc.index(feat)\n",
    "                except ValueError:\n",
    "                    rank = 0\n",
    "                if yi == 0:\n",
    "                    rank_f = n_feats - rank_f\n",
    "                    rank = n_feats - rank\n",
    "                f_ranks.append(rank_f)\n",
    "                expl_ranks.append(rank)\n",
    "            score += scipy.stats.spearmanr(expl_ranks, f_ranks)[0]\n",
    "        score /= len(explanations)\n",
    "        print('fidelity_g', explainer, task, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_features = P.problem.biased_features\n",
    "n_feats = P.X_test.shape[1]\n",
    "\n",
    "for explainer, expl_expl_data in explainer_data.items():\n",
    "    for task, expl_expl_task_data in expl_expl_data.items():\n",
    "        explanations = expl_expl_task_data['explanations']\n",
    "        y_test_pred_f = expl_expl_task_data['y_test_pred_f_biased']\n",
    "        if y_test_pred_f is None:\n",
    "            y_test_pred_f = expl_expl_task_data['y_test_pred']\n",
    "        score = 0\n",
    "        for yi, expl in zip(y_test_pred_f, explanations):\n",
    "            expl = {k.rsplit('=', 1)[0]: v for k, v in expl}\n",
    "            # ascending\n",
    "            expl_keys_asc = sorted(expl.keys(), key=lambda x: expl[x])\n",
    "            expl_ranks = []\n",
    "            for feat in biased_features:\n",
    "                try:\n",
    "                    rank = expl_keys_asc.index(feat)\n",
    "                except ValueError:\n",
    "                    rank = 0\n",
    "                if yi == 0:\n",
    "                    rank = n_feats - rank\n",
    "                expl_ranks.append(rank)\n",
    "            score += np.mean(expl_ranks)\n",
    "        score /= len(explanations) * n_feats\n",
    "        print('fidelity_g(precision)', explainer, task, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defending Explainer Explanations\n",
    "This block uses the defense approach with each of the explainers to defend against the attack. Explanation fidelity is restored when our approach is employed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c61c79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explainer_data_defense = generate_explanations(\n",
    "    C, P,\n",
    "    robustness_model=detectors,\n",
    "    # num_samples_explain=...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defense Evaluation Metrics\n",
    "The gathered results for attack defense are shown in the subsequent blocks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats = P.X_test.shape[1]\n",
    "for explainer, expl_expl_data in explainer_data.items():\n",
    "    g0_explanations = explainer_data[explainer][None]['explanations']\n",
    "    for task, expl_expl_task_data in expl_expl_data.items():\n",
    "        g_explanations = expl_expl_task_data['explanations']\n",
    "        err_expls = 0\n",
    "        for expl_g, expl_h in zip(g0_explanations, g_explanations):\n",
    "            expl_g, expl_h = dict(expl_g), dict(expl_h)\n",
    "            for feat in {*expl_g.keys()} | {*expl_h.keys()}:\n",
    "                contrib_g = expl_g.get(feat, 0.)\n",
    "                contrib_h = expl_h.get(feat, 0.)\n",
    "                err_expls += (contrib_h - contrib_g) ** 2\n",
    "        err_expls /= len(g_explanations) * n_feats\n",
    "        print('infidelity_g_wrt_g', explainer, task, err_expls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats = P.X_test.shape[1]\n",
    "for explainer, expl_expl_data in explainer_data_defense.items():\n",
    "    g_explanations = explainer_data[explainer][None]['explanations']\n",
    "    for task, expl_expl_task_data in expl_expl_data.items():\n",
    "        h_explanations = expl_expl_task_data['explanations']\n",
    "        assert len(g_explanations) == len(h_explanations)\n",
    "        err_expls = 0\n",
    "        for expl_g, expl_h in zip(g_explanations, h_explanations):\n",
    "            expl_g, expl_h = dict(expl_g), dict(expl_h)\n",
    "            for feat in {*expl_g.keys()} | {*expl_h.keys()}:\n",
    "                contrib_g = expl_g.get(feat, 0.)\n",
    "                contrib_h = expl_h.get(feat, 0.)\n",
    "                err_expls += (contrib_h - contrib_g) ** 2\n",
    "        err_expls /= len(g_explanations) * n_feats\n",
    "        print('infidelity_CAD-DEFENSE_wrt_g', explainer, task, err_expls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_features = P.problem.biased_features\n",
    "n_feats = P.X_test.shape[1]\n",
    "\n",
    "for explainer, expl_expl_data in explainer_data_defense.items():\n",
    "    for task, expl_expl_task_data in expl_expl_data.items():\n",
    "        explanations = expl_expl_task_data['explanations']\n",
    "        y_test_pred_f = expl_expl_task_data['y_test_pred_f_biased']\n",
    "        if y_test_pred_f is None:\n",
    "            y_test_pred_f = expl_expl_task_data['y_test_pred']\n",
    "        score = 0\n",
    "        for yi, expl in zip(y_test_pred_f, explanations):\n",
    "            expl = {k.rsplit('=', 1)[0]: v for k, v in expl}\n",
    "            # ascending\n",
    "            expl_keys_asc = sorted(expl.keys(), key=lambda x: expl[x])\n",
    "            f_ranks = []\n",
    "            expl_ranks = []\n",
    "            for feat in biased_features:\n",
    "                f_ranks.append(n_feats - 1)\n",
    "                try:\n",
    "                    expl_ranks.append(expl_keys_asc.index(feat))\n",
    "                except ValueError:\n",
    "                    expl_ranks.append(0)\n",
    "            for feat in biased_features:\n",
    "                rank_f = n_feats - 1\n",
    "                try:\n",
    "                    rank = expl_keys_asc.index(feat)\n",
    "                except ValueError:\n",
    "                    rank = 0\n",
    "                if yi == 0:\n",
    "                    rank_f = n_feats - rank_f\n",
    "                    rank = n_feats - rank\n",
    "                f_ranks.append(rank_f)\n",
    "                expl_ranks.append(rank)\n",
    "            for feat in {*P.features} - {*biased_features}:\n",
    "                rank_f = 0\n",
    "                try:\n",
    "                    rank = expl_keys_asc.index(feat)\n",
    "                except ValueError:\n",
    "                    rank = 0\n",
    "                if yi == 0:\n",
    "                    rank_f = n_feats - rank_f\n",
    "                    rank = n_feats - rank\n",
    "                f_ranks.append(rank_f)\n",
    "                expl_ranks.append(rank)\n",
    "            score += scipy.stats.spearmanr(expl_ranks, f_ranks)[0]\n",
    "        score /= len(explanations)\n",
    "        print('fidelity_CAD-DEFENSE', explainer, task, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unfooling",
   "language": "python",
   "name": "unfooling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
